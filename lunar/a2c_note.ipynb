{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "# from pyvirtualdisplay import Display\n",
    "\n",
    "from gym import wrappers\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models\n",
    "import os\n",
    "from tensorflow.keras import backend as K\n",
    "%matplotlib inline\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# for gpu in gpus:\n",
    "#     tf.config.experimental.set_memory_growth(gpu, True)\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "print(\"*****************\")\n",
    "print(gpus)\n",
    "print(\"*****************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "# env = gym.wrappers.Monitor(env,\"./notefiles/test\",force=True)\n",
    "action_space = env.action_space.n\n",
    "observation_space = env.observation_space.shape[0]\n",
    "\n",
    "scores = []\n",
    "\n",
    "rewards=[]\n",
    "\n",
    "obs = env.reset()\n",
    "cum_reward = 0\n",
    "frames = []\n",
    "# fig = plt.figure()\n",
    "#         obs = env.reset()\n",
    "#         episode_reward = 0\n",
    "while True:\n",
    "    # f=plt.imshow(env.render(mode = 'rgb_array'),animated=True)\n",
    "    # frames.append([f])\n",
    "\n",
    "#         q_values = agent.get_q(obs)\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "#             print(t,action,reward)\n",
    "    cum_reward += reward\n",
    "#     print(reward)\n",
    "    rewards.append(reward)\n",
    "#     print(done)\n",
    "    if done:\n",
    "        break\n",
    "scores.append(cum_reward)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORYLEN=int(1e5)\n",
    "# MEMORYLEN=int(10000)\n",
    "MEMORYLEN=int(64)\n",
    "BATCHSIZE=64\n",
    "EPOCHS=1\n",
    "\n",
    "\n",
    "class DQNAgent():\n",
    "    def __init__(self,actions=4,obs=8):\n",
    "        self.actions=actions\n",
    "        self.observations=obs\n",
    "        self.actor,self.critic,self.policy=self.load_model()\n",
    "        \n",
    "        \n",
    "#         self.copy_weights()\n",
    "\n",
    "        self.memory=deque(maxlen=MEMORYLEN)\n",
    "        self.gamma=0.99\n",
    "        self.patience=0\n",
    "        \n",
    "           \n",
    "    def play(self,observation,epsilon):\n",
    "        if (len(self.memory)<BATCHSIZE):\n",
    "            \n",
    "            action=np.random.randint(low=0,high=self.actions)\n",
    "            return action\n",
    "        else:\n",
    "            action=self.choose_action(observation)\n",
    "            return action\n",
    "    \n",
    "    def choose_action(self,observation):\n",
    "        state=observation[np.newaxis,:]\n",
    "        action_probs=self.policy.predict(state)[0]\n",
    "        action=np.random.choice(self.actions,p=action_probs)\n",
    "        return action\n",
    "            \n",
    "    def step(self,state, action, reward, next_state, done):\n",
    "        self.memory.append([state, action, reward, next_state, done])\n",
    "        if ((len(self.memory)>=BATCHSIZE) ):\n",
    "            self.train_model()\n",
    "        pass\n",
    " \n",
    "    \n",
    "    def train_model(self):\n",
    "        \n",
    "        rnd_indices = np.random.choice(len(self.memory), size=BATCHSIZE)\n",
    "        data=np.array(self.memory)[rnd_indices]\n",
    "        np.random.shuffle(data)\n",
    "        \n",
    "        state, action, reward, next_state, done=np.stack(data[:,0]),np.stack(data[:,1]),np.stack(data[:,2]),np.stack(data[:,3]),np.stack(data[:,4])\n",
    "#         state=state[np.newaxis,:]\n",
    "#         next_state=next_state[np.newaxis,:]\n",
    "        \n",
    "        critic_output_state=self.critic.predict(state).flatten()\n",
    "        critic_output_next_state=self.critic.predict(next_state).flatten()\n",
    "        \n",
    "        target=reward+self.gamma*critic_output_next_state*(1-done)\n",
    "        delta=target-critic_output_state\n",
    "        \n",
    "        \n",
    "        actions=np.zeros([BATCHSIZE,self.actions])\n",
    "        actions[np.arange(BATCHSIZE),action]=1\n",
    "        \n",
    "        self.actor.fit([state,delta],actions,verbose=0)\n",
    "        self.critic.fit(state,target,verbose=0)\n",
    "                \n",
    "#         self.patience+=1\n",
    "#         if self.patience==10:\n",
    "#             self.copy_weights()\n",
    "#             self.patience=0\n",
    "        \n",
    "        pass\n",
    "    def model_predictions(self,observation):\n",
    "        pred=self.model.predict(observation.reshape(1,-1))\n",
    "        pred=np.argmax(pred)\n",
    "        return pred\n",
    "        \n",
    "    def load_model(self):\n",
    "        num_input = layers.Input(shape=(self.observations, ))\n",
    "        delta=layers.Input(shape=[1])\n",
    "        dense1=layers.Dense(128,activation='relu')(num_input)\n",
    "        dense2=layers.Dense(64,activation='relu')(dense1)\n",
    "        probs=layers.Dense(self.actions,activation='softmax')(dense2)\n",
    "        values=layers.Dense(1,activation='linear')(dense2)\n",
    "        def custom_loss(y_true,y_pred):\n",
    "            out=K.clip(y_pred,1e-8,1-1e-8)\n",
    "            log_lik=y_true*K.log(out)\n",
    "            return K.sum(-log_lik*delta)\n",
    "        \n",
    "        actor=models.Model(inputs=[num_input,delta],outputs=[probs])\n",
    "        actor.compile(loss=custom_loss,optimizer=tf.keras.optimizers.Adam(lr=0.001,decay=0.005))\n",
    "        \n",
    "        critic=models.Model(inputs=[num_input],outputs=[values])\n",
    "        critic.compile(loss=\"mse\",optimizer=tf.keras.optimizers.Adam(lr=0.001,decay=0.005))\n",
    "        \n",
    "        policy=models.Model(inputs=[num_input],outputs=[probs])\n",
    "        \n",
    "        return actor,critic,policy\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "starttime=time.time()\n",
    "scores = []                        # list containing scores from each episode\n",
    "scores_window = deque(maxlen=100)  # last 100 scores\n",
    "n_episodes=20\n",
    "agent=DQNAgent()\n",
    "\n",
    "max_t=500\n",
    "eps_start=1.0\n",
    "eps_end=0.2\n",
    "eps_decay=0.995\n",
    "\n",
    "eps = eps_start\n",
    "env=gym.make('LunarLander-v2')\n",
    "eps_history=[]\n",
    "for i_episode in range(1, n_episodes+1):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    for i_ in range(1,max_t+1):\n",
    "        action = agent.play(state,eps)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "#         if reward==-100:\n",
    "#             reward=-100\n",
    "#         if reward==100:\n",
    "#             reward=35\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if done:\n",
    "            break \n",
    "    scores_window.append(score)       # save most recent score\n",
    "    scores.append(score)              # save most recent score\n",
    "    eps = max(eps_end, eps_decay*eps)\n",
    "    eps_history.append(eps)\n",
    "    if i_episode % 10 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "#             agent.policy.save(\"weightsfolder/policy_.h5\")\n",
    "    if np.mean(scores_window)>=190.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            \n",
    "            break\n",
    "endtime= time.time() \n",
    "print(endtime-starttime)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "def gen_gif(agent,fname):\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    # env = gym.wrappers.Monitor(env,\"/anotherdrive/projects/gym/lunar/notefiles/test\",force=True)\n",
    "    action_space = env.action_space.n\n",
    "    observation_space = env.observation_space.shape[0]\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    rewards=[]\n",
    "\n",
    "    state = env.reset()\n",
    "    cum_reward = 0\n",
    "    frames = []\n",
    "    fig = plt.figure()\n",
    "  \n",
    "    while True:\n",
    "        f=plt.imshow(env.render(mode = 'rgb_array'),animated=True)\n",
    "        frames.append([f])\n",
    "\n",
    "    #         q_values = agent.get_q(obs)\n",
    "        action = agent.play(state,0)\n",
    "\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "    #             print(t,action,reward)\n",
    "        cum_reward += reward\n",
    "    #     print(reward)\n",
    "        rewards.append(reward)\n",
    "    #     print(done)\n",
    "        if done:\n",
    "            break\n",
    "    scores.append(cum_reward)\n",
    "    env.close()\n",
    "    print(cum_reward)    \n",
    "    ani = animation.ArtistAnimation(fig, frames, interval=200, blit=True,\n",
    "                                repeat_delay=0)\n",
    "\n",
    "    ani.save('/anotherdrive/projects/gym/lunar/notefiles/gifs/lunar_{}_.gif'.format(fname),writer=\"PillowWriter\")\n",
    "#     plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_gif(agent,\"test01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "starttime=time.time()\n",
    "scores = []                        # list containing scores from each episode\n",
    "scores_window = deque(maxlen=100)  # last 100 scores\n",
    "n_episodes=4000\n",
    "agent=DQNAgent()\n",
    "\n",
    "max_t=500\n",
    "eps_start=1.0\n",
    "eps_end=0.2\n",
    "eps_decay=0.995\n",
    "\n",
    "eps = eps_start\n",
    "env=gym.make('LunarLander-v2')\n",
    "eps_history=[]\n",
    "best_score=-9999\n",
    "for i_episode in range(1, n_episodes+1):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    for i_ in range(1,max_t+1):\n",
    "        action = agent.play(state,eps)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "#         if reward==-100:\n",
    "#             reward=-100\n",
    "#         if reward==100:\n",
    "#             reward=35\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if done:\n",
    "            break \n",
    "    scores_window.append(score)       # save most recent score\n",
    "    scores.append(score)              # save most recent score\n",
    "    eps = max(eps_end, eps_decay*eps)\n",
    "    eps_history.append(eps)\n",
    "    if i_episode % 10 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f} Epsilon: {}'.format(i_episode, np.mean(scores_window),eps))\n",
    "#             agent.policy.save(\"weightsfolder/policy_.h5\")\n",
    "    if np.mean(scores_window)>=best_score:\n",
    "        best_score=np.mean(scores_window)\n",
    "        print(\"new best_score: {}\".format(best_score))\n",
    "        gen_gif(agent,i_episode)\n",
    "        agent.policy.save(\"/anotherdrive/projects/gym/lunar/notefiles/weights/lunar_{}_.h5\".format(i_episode))\n",
    "    if np.mean(scores_window)>=190.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            \n",
    "            break\n",
    "endtime= time.time() \n",
    "print(endtime-starttime)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
